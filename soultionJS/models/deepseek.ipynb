{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement aimo (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for aimo\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom client with custom model loader\n",
    "\n",
    "class BertModelClient:\n",
    "    def __init__(self, config, **kwargs):\n",
    "        print(f\"CustomModelClient config: {config}\")\n",
    "        self.device = config.get(\"device\", \"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config[\"model\"]).to(self.device)\n",
    "        self.model_name = config[\"model\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config[\"model\"], use_fast=False)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        # params are set by the user and consumed by the user since they are providing a custom model\n",
    "        # so anything can be done here\n",
    "        gen_config_params = config.get(\"params\", {})\n",
    "        self.max_length = gen_config_params.get(\"max_length\", 256)\n",
    "\n",
    "        print(f\"Loaded model {config['model']} to {self.device}\")\n",
    "\n",
    "    def create(self, params):\n",
    "        if params.get(\"stream\", False) and \"messages\" in params:\n",
    "            raise NotImplementedError(\"Local models do not support streaming.\")\n",
    "        else:\n",
    "            num_of_responses = params.get(\"n\", 1)\n",
    "\n",
    "            # can create my own data response class\n",
    "            # here using SimpleNamespace for simplicity\n",
    "            # as long as it adheres to the ClientResponseProtocol\n",
    "\n",
    "            response = SimpleNamespace()\n",
    "\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                params[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True\n",
    "            ).to(self.device)\n",
    "            inputs_length = inputs.shape[-1]\n",
    "\n",
    "            # add inputs_length to max_length\n",
    "            max_length = self.max_length + inputs_length\n",
    "            generation_config = GenerationConfig(\n",
    "                max_length=max_length,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            response.choices = []\n",
    "            response.model = self.model_name\n",
    "\n",
    "            for _ in range(num_of_responses):\n",
    "                outputs = self.model.generate(inputs, generation_config=generation_config)\n",
    "                # Decode only the newly generated text, excluding the prompt\n",
    "                text = self.tokenizer.decode(outputs[0, inputs_length:])\n",
    "                choice = SimpleNamespace()\n",
    "                choice.message = SimpleNamespace()\n",
    "                choice.message.content = text\n",
    "                choice.message.function_call = None\n",
    "                response.choices.append(choice)\n",
    "\n",
    "            return response\n",
    "\n",
    "    def message_retrieval(self, response):\n",
    "        \"\"\"Retrieve the messages from the response.\"\"\"\n",
    "        choices = response.choices\n",
    "        return [choice.message.content for choice in choices]\n",
    "\n",
    "    def cost(self, response) -> float:\n",
    "        \"\"\"Calculate the cost of the response.\"\"\"\n",
    "        response.cost = 0\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_usage(response):\n",
    "        # returns a dict of prompt_tokens, completion_tokens, total_tokens, cost, model\n",
    "        # if usage needs to be tracked, else None\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_REPETITIONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mMathProblemSolver\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m, in \u001b[0;36mMathProblemSolver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError selecting random instructions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep 1: ..., Step 2: ...\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mboxed\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0-99}\u001b[39m\u001b[38;5;124m'\u001b[39m}]\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer_multiple_times\u001b[39m(\u001b[38;5;28mself\u001b[39m, question, label_type, csv_file, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[43mN_REPETITIONS\u001b[49m):\n\u001b[1;32m     72\u001b[0m     exemplars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_random_instructions(csv_file, label_type)\n\u001b[1;32m     73\u001b[0m     exemplar_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexemplar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_into_steps(exemplar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexemplar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m exemplar \u001b[38;5;129;01min\u001b[39;00m exemplars\n\u001b[1;32m     78\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_REPETITIONS' is not defined"
     ]
    }
   ],
   "source": [
    "class MathProblemSolver:\n",
    "    def __init__(self, model_name, device):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer, self.model = self.load_model_and_tokenizer()\n",
    "        self.generation_config = self.load_generation_config()\n",
    "        self.tokenizerBERT, self.modelBERT = self.load_bert_model()\n",
    "        self.label_mapping = {\n",
    "            0: \"Algebra\",\n",
    "            1: \"Counting & Probability\",\n",
    "            2: \"Geometry\",\n",
    "            3: \"Intermediate Algebra\",\n",
    "            4: \"Number Theory\",\n",
    "            5: \"Prealgebra\",\n",
    "            6: \"Precalculus\"\n",
    "        }\n",
    "          \n",
    "    def load_model_and_tokenizer(self):\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "                device_map='auto',\n",
    "                offload_folder=\"offload\"\n",
    "            )\n",
    "            return tokenizer, model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model/tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_generation_config(self):\n",
    "        try:\n",
    "            generation_config = GenerationConfig.from_pretrained(self.model_name)\n",
    "            generation_config.pad_token_id = self.tokenizer.eos_token_id\n",
    "            return generation_config\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading generation config: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def load_bert_model(self):\n",
    "        modelname= '/kaggle/input/bert-classifier-math/transformers/prefinetuned/1/bert-finetuned-math-prob-classification'\n",
    "        tokenizerBERT = AutoTokenizer.from_pretrained(modelname)\n",
    "        modelBERT = AutoModelForSequenceClassification.from_pretrained(modelname)\n",
    "        modelBERT.to(self.device)\n",
    "        return tokenizerBERT, modelBERT\n",
    "\n",
    "    def split_into_steps(self, text):\n",
    "        try:\n",
    "            sentences = text.split('. ')\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "            steps = [f\"Step {i+1}: {sentence}\" for i, sentence in enumerate(sentences)]\n",
    "            return steps\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting instructions: {e}\")\n",
    "            return text\n",
    "\n",
    "    def select_random_instructions(self, csv_file, class_label):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df = df[pd.to_numeric(df['answer'], errors='coerce').notnull()]\n",
    "            df = df.drop_duplicates(subset=['problem'])\n",
    "            filtered_df = df[df['Type'] == class_label.lower()]\n",
    "            selected_rows = filtered_df.groupby('Type').head(2)\n",
    "            results = [{'question': row['problem'], 'instruction': row['steps'], 'answer': row['answer']} for _, row in selected_rows.iterrows()]\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error selecting random instructions: {e}\")\n",
    "            return [{'question': '...', 'instruction': 'Step 1: ..., Step 2: ...', 'answer': '\\\\boxed{0-99}'}]\n",
    "\n",
    "    def generate_answer_multiple_times(self, question, label_type, csv_file, num_iterations=N_REPETITIONS):\n",
    "        exemplars = self.select_random_instructions(csv_file, label_type)\n",
    "        exemplar_texts = \"\\n\\n\".join(\n",
    "            f\"Q: {exemplar['question']}\\n\"\n",
    "            f\"Steps: {self.split_into_steps(exemplar['instruction'])}\\n\"\n",
    "            f\"A: {exemplar['answer']}\"\n",
    "            for exemplar in exemplars\n",
    "        )\n",
    "\n",
    "        prompt = (\n",
    "            'You are going to solve math problems that have a positive integer solution. Keep logic concise.'\n",
    "            f\"Here is a math problem you are to solve (positive numerical answer): {question}\\n\"\n",
    "            f\"This particular question is a {label_type} question.\\n\"\n",
    "            \"To solve it, first determine a series of logical steps for solving the problem and then follow these steps. It is imperative that the final answer is in the brackets of \\\\boxed{}. \\n\\n\"\n",
    "            \"The final answer should be a positive integer and not an algebraic expression.\\n\"\n",
    "            f\"Here are some examples of how to solve similar {label_type} problems step-by-step:\\n\\n\"\n",
    "            f\"{exemplar_texts}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        answers = []\n",
    "        for _ in range(num_iterations):\n",
    "            try:\n",
    "                inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        inputs['input_ids'],\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        num_beams=3,\n",
    "                        early_stopping=True,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        attention_mask=inputs['attention_mask'],\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        top_k=50\n",
    "                    )\n",
    "\n",
    "                result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#                 match = re.search(r'Answer:\\s*(.*)', result, re.DOTALL)\n",
    "                match = self.process_text_output(result)\n",
    "                if match:\n",
    "                    answers.append(match)\n",
    "#                     answers.append(self.extract_boxed_answer(match.group(1).strip()))\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "\n",
    "        return answers\n",
    "\n",
    "#     def extract_boxed_answer(self, generated_text):\n",
    "#         try:\n",
    "#             match = re.search(r'\\\\boxed{([^}]*)}', generated_text)\n",
    "#             if match:\n",
    "#                 return match.group(1)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error extracting boxed answer: {e}\")\n",
    "#         return 0  \n",
    "    \n",
    "    def naive_parse(self, answer):\n",
    "        try:\n",
    "            out = []\n",
    "            for l in list(answer):\n",
    "                if l in '0123456789':\n",
    "                    out.append(l)\n",
    "            return ''.join(out)\n",
    "        except Exception as e:\n",
    "            return -1\n",
    "\n",
    "    def process_text_output(self, output):\n",
    "        try:\n",
    "            result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', output)\n",
    "            if not result_output:  #no boxed answer given\n",
    "                result_output = self.naive_parse(output)\n",
    "            else:\n",
    "                result_output = result_output[-1] #last instance of boxed answer\n",
    "\n",
    "#             if not result_output:\n",
    "#                 result_output = -1\n",
    "            \n",
    "            result_output = round(float(eval(result_output))) % 1000\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            result_output = -1\n",
    "        return result_output    # int\n",
    "\n",
    "    def aggregate_answers(self, answers):\n",
    "        answer_counts = {}\n",
    "        for answer in answers:\n",
    "            if answer in answer_counts:\n",
    "                answer_counts[answer] += 1\n",
    "            else:\n",
    "                answer_counts[answer] = 1\n",
    "        sorted_answers = sorted(answer_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        return sorted_answers[0] if sorted_answers else (None, 0)\n",
    "\n",
    "    def manage_context_and_generate_answers(self, question, context_prompt, total_tokens, conversation, label_type, csv_file):\n",
    "        try:\n",
    "            answers = self.generate_answer_multiple_times(question, label_type, csv_file) #list of integers\n",
    "            best_answer, count = self.aggregate_answers(answers) \n",
    "#             best_generated_text = best_answer\n",
    "#             boxed_answer = self.process_text_output(best_answer)\n",
    "            qna_text = f\"Question: {question}\\nAnswer: {best_answer}\\n\\n\"\n",
    "            conversation += qna_text\n",
    "            total_tokens += len(self.tokenizer.encode(qna_text))\n",
    "\n",
    "            if total_tokens >= 4000:\n",
    "                conversation += context_prompt + \"\\n\\n\"\n",
    "                total_tokens = len(self.tokenizer.encode(conversation))\n",
    "\n",
    "#             result = {\n",
    "# #                 'generated_text': answers,\n",
    "#                 'int_answer': best_answer,\n",
    "# #                 'total_tokens': total_tokens\n",
    "#             }\n",
    "            result = best_answer\n",
    "            print(result)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {'generated_text': '', 'int_answer': 1, 'total_tokens': 4001}\n",
    "\n",
    "    def predict(self, text):\n",
    "        try:\n",
    "            inputs = self.tokenizerBERT(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.modelBERT(**inputs)\n",
    "\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "            return self.label_mapping[predicted_class]\n",
    "        except Exception as e:\n",
    "            predicted_class = random.randint(0,6)\n",
    "            return self.label_mapping[predicted_class]\n",
    "\n",
    "    def flush(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aimo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m full_conversation \u001b[38;5;241m=\u001b[39m context_prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Set up the evaluation API\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maimo\u001b[39;00m\n\u001b[1;32m     30\u001b[0m env \u001b[38;5;241m=\u001b[39m aimo\u001b[38;5;241m.\u001b[39mmake_env()\n\u001b[1;32m     31\u001b[0m iter_test \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39miter_test()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aimo'"
     ]
    }
   ],
   "source": [
    "context_prompt = \"\"\"\n",
    "This is a compendium of things which we would normally explain in maths competitions for teenaged schoolchildren as context for you to better understand the task at hand:\n",
    "\n",
    "\\\\begin{enumerate}\n",
    "    \\\\item The final answer should be a positive integer and should be placed within $\\\\boxed{}$.\n",
    "    \\\\item Established mathematical notation will be used.\n",
    "    \\\\item We might use a colon or a vertical line as a separator in set notation so $\\\\{x \\\\mid x \\\\in \\\\mathbb{Z}, x > 0\\\\} = \\\\{y : y \\\\in \\\\mathbb{Z}, y > 0\\\\}$.\n",
    "    \\\\item Floor and ceiling notation, so for $x$ a real number we let $\\\\lfloor x \\\\rfloor = \\\\max\\\\{z \\\\mid x \\\\in \\\\mathbb{Z}, z \\\\leq x\\\\}$. Similarly $\\\\lceil x \\\\rceil = \\\\min\\\\{z \\\\mid x \\\\in \\\\mathbb{Z}, z \\\\geq x\\\\}$.\n",
    "    \\\\item Fractional part notation. If $x$ is a real number, we define $\\\\{x\\\\}$ to mean $x - \\\\lfloor x \\\\rfloor$.\n",
    "    \\\\item We write a line over a non-negative integer written in base 10 notation to indicate that it is being viewed as a string of digits rather than a number. Thus the second digit of $\\\\overline{1729}$ is 7 but 1729 does not have a second digit because it is an integer.\n",
    "    \\\\item We allow a phrase such as \"$x$ is a 3-digit positive integer\" to mean that if written in Arabic notation as $x = a_m \\\\cdot a_{m-1} \\\\cdot \\\\ldots \\\\cdot a_1$ with $a_i$ all digits and $a_m \\\\neq 0$, then $n = m$. We allow \"the sum of the digits of $n$\" to mean: write $n$ in Arabic base 10 notation and then sum the digits.\n",
    "    \\\\item We allow informal probability language such as: \"a point is chosen uniformly at random in the interval $[0, 1]$\".\n",
    "    \\\\item We use $\\\\binom{n}{r}$ to denote the number of ways of choosing $r$ things from $n$ things.\n",
    "    \\\\item The sum over the empty set is 0 and the product over the empty set is 1.\n",
    "    \\\\item We use an ellipsis to denote an obvious pattern, either on the line of print of midline (as appropriate) so the set of the first $n$ positive integers can be written $\\\\{1, 2, \\\\ldots, n\\\\}$ and their sum is $1 + 2 + \\\\ldots + n$.\n",
    "    \\\\item For integers $l$, $m$, $n$ then $l$ raised by $m$ which is raised by $n$ denotes $l^{(m^n)}$.\n",
    "    \\\\item $m^0 = 1$ for all integers $m$ (including 0) if doing combinatorial enumeration. If $x$ is real then $x^0$ needs to be clarified if $x = 0$.\n",
    "    \\\\item British or American versions of English can be used. Thus \"highest common factor\" means the same as \"greatest common divisor\".\n",
    "    \\\\item A prefix or subscript may be used to indicate features of a triangle associated with vertices. Thus triangle $ABC$ has three altitudes, and the one dropped from $A$ could be denoted the altitude through $A$, the $A$-altitude or the altitude $h_a$. Similarly for median lines.\n",
    "    \\\\item If the term natural number is used, then it will be made clear if 0 is a natural number.\n",
    "    \\\\item $:=$ means 'is defined to be equal to'.\n",
    "\\\\end{enumerate>\"\"\"\n",
    "\n",
    "total_tokens = 0\n",
    "full_conversation = context_prompt + \"\\n\\n\"\n",
    "\n",
    "# Set up the evaluation API\n",
    "import aimo\n",
    "\n",
    "env = aimo.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "inputfile= '/kaggle/input/prmclean/prmclean.csv'\n",
    "solver = MathProblemSolver(MODEL_NAME, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
