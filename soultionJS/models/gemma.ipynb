{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.12.0)\n",
      "Requirement already satisfied: sympy in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.2.5-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from kagglehub) (24.0)\n",
      "Requirement already satisfied: requests in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from kagglehub) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->kagglehub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kolbeweathington/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages (from requests->kagglehub) (2024.2.2)\n",
      "Using cached kagglehub-0.2.5-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/james113001/bert-classifier-math/transformers/prefinetuned/1/download...\n",
      "100%|██████████| 775M/775M [01:23<00:00, 9.69MB/s] \n",
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: /Users/kolbeweathington/.cache/kagglehub/models/james113001/bert-classifier-math/transformers/prefinetuned/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"james113001/bert-classifier-math/transformers/prefinetuned\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "GemmaConfig={\n",
    "    \"model\" : \"../inputs/gemma/transformers/1.1-2b-it/1\",\n",
    "    \"device\"  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \"api_key\": \"EMPTY\", \n",
    "    \"max_tokens\":1000,\n",
    "    \"base_url\":f\"http://localhost:11434/v1\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autogen.ipynb   bert.ipynb      deepseek.ipynb  gemma.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "class GemmaModelClient:\n",
    "    def __init__(self, config, **kwargs):\n",
    "        print(f\"GemmaModelClient config: {config}\")\n",
    "        self.device = config.get(\"device\", \"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config[\"model\"]).to(self.device)\n",
    "        self.model_name = config[\"model\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config[\"model\"], use_fast=False)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "\n",
    "        # params are set by the user and consumed by the user since they are providing a custom model\n",
    "        # so anything can be done here\n",
    "        gen_config_params = config.get(\"params\", {})\n",
    "        self.max_length = gen_config_params.get(\"max_length\", 256)\n",
    "\n",
    "        print(f\"Loaded model {config['model']} to {self.device}\")\n",
    "\n",
    "        \n",
    "\n",
    "    # def create(self, params):\n",
    "        if params.get(\"stream\", False) and \"messages\" in params:\n",
    "            raise NotImplementedError(\"Local models do not support streaming.\")\n",
    "        else:\n",
    "            num_of_responses = params.get(\"n\", 1)\n",
    "\n",
    "            # can create my own data response class\n",
    "            # here using SimpleNamespace for simplicity\n",
    "            # as long as it adheres to the ClientResponseProtocol\n",
    "\n",
    "            response = SimpleNamespace()\n",
    "\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                params[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True\n",
    "            ).to(self.device)\n",
    "            inputs_length = inputs.shape[-1]\n",
    "\n",
    "            # add inputs_length to max_length\n",
    "            max_length = self.max_length + inputs_length\n",
    "            generation_config = GenerationConfig(\n",
    "                max_length=max_length,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            response.choices = []\n",
    "            response.model = self.model_name\n",
    "\n",
    "            for _ in range(num_of_responses):\n",
    "                outputs = self.model.generate(inputs, generation_config=generation_config)\n",
    "                # Decode only the newly generated text, excluding the prompt\n",
    "                text = self.tokenizer.decode(outputs[0, inputs_length:])\n",
    "                choice = SimpleNamespace()\n",
    "                choice.message = SimpleNamespace()\n",
    "                choice.message.content = text\n",
    "                choice.message.function_call = None\n",
    "                response.choices.append(choice)\n",
    "\n",
    "            return response\n",
    "\n",
    "    # def message_retrieval(self, response):\n",
    "    #     \"\"\"Retrieve the messages from the response.\"\"\"\n",
    "    #     choices = response.choices\n",
    "    #     return [choice.message.content for choice in choices]\n",
    "\n",
    "    # def cost(self, response) -> float:\n",
    "    #     \"\"\"Calculate the cost of the response.\"\"\"\n",
    "    #     response.cost = 0\n",
    "    #     return 0\n",
    "\n",
    "    # @staticmethod\n",
    "    # def get_usage(response):\n",
    "        # returns a dict of prompt_tokens, completion_tokens, total_tokens, cost, model\n",
    "        # if usage needs to be tracked, else None\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaModelClient config: {'model': '../inputs/gemma/transformers/1.1-2b-it/1', 'cpuapi_key': 'EMPTY', 'max_tokens': 1000, 'base_url': 'http://localhost:11434/v1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.01s/it]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nGemmaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[43mGemmaModelClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGemmaConfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an ai with a simple goal. you will read a math problem written in latex. Then reread the problem this time to fully understand what it is asking you to solve. then create a procedure for another ai to follow and solve to get the correct solution.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m prompt\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36mGemmaModelClient.__init__\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# params are set by the user and consumed by the user since they are providing a custom model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# so anything can be done here\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:880\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    878\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m         )\n\u001b[0;32m--> 880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages/transformers/utils/import_utils.py:1475\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1475\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/homeworkmachine/.conda/lib/python3.11/site-packages/transformers/utils/import_utils.py:1463\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1461\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nGemmaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "solver = GemmaModelClient(GemmaConfig)\n",
    "prompt = \"You are an ai with a simple goal. you will read a math problem written in latex. Then reread the problem this time to fully understand what it is asking you to solve. then create a procedure for another ai to follow and solve to get the correct solution.\"\n",
    "prompt\n",
    "# def seperate_into_steps(model,text):\n",
    "#     try:\n",
    "#             inputs = model.tokenizerBERT(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#             inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.modelBERT(**inputs)\n",
    "\n",
    "#             predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "#             predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "#             return self.label_mapping[predicted_class]\n",
    "#         except Exception as e:\n",
    "#             predicted_class = random.randint(0,6)\n",
    "#             return self.label_mapping[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def split_into_steps(text):\n",
    "        try:\n",
    "            sentences = text.split('. ')\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "            steps = [f\"Step {i+1}: {sentence}\" for i, sentence in enumerate(sentences)]\n",
    "            return steps\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting instructions: {e}\")\n",
    "            return text\n",
    "\n",
    "    def select_random_instructions(self, csv_file, class_label):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df = df[pd.to_numeric(df['answer'], errors='coerce').notnull()]\n",
    "            df = df.drop_duplicates(subset=['problem'])\n",
    "            filtered_df = df[df['Type'] == class_label.lower()]\n",
    "            selected_rows = filtered_df.groupby('Type').head(2)\n",
    "            results = [{'question': row['problem'], 'instruction': row['steps'], 'answer': row['answer']} for _, row in selected_rows.iterrows()]\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error selecting random instructions: {e}\")\n",
    "            return [{'question': '...', 'instruction': 'Step 1: ..., Step 2: ...', 'answer': '\\\\boxed{0-99}'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "                device_map='auto',\n",
    "                offload_folder=\"offload\"\n",
    "            )\n",
    "            return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
