{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
    "This starter notebook is provided by the Keras team.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# AI Math Olympiad with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://i.ibb.co/9rx4pbX/AIMO.png\">\n",
    "</div>\n",
    "\n",
    "In this competition, we aim is to build AI models that can solve tough math problems, in other words, creating LLM models capable of solving Math Olympiad problems. This notebook will guide you through the process of fine-tuning the **Gemma** LLM model with LoRA to solve math problems using KerasNLP. With KerasNLP, fine-tuning with LoRA becomes straightforward with just a few lines of code.\n",
    "\n",
    "**Did you know:**: This notebook is backend-agnostic? Which means it supports TensorFlow, PyTorch, and JAX backends. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n",
    "\n",
    "**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries\n",
    "\n",
    "We need to install latest KerasNLP to load Gemma 1.1 model. As we don't have access to internet during inference, we will be installing this library from our local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade keras  # Upgrade to Keras 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (2.31.0)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.64.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.16.1-cp312-cp312-macosx_12_0_arm64.whl (227.1 MB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.64.0-cp312-cp312-macosx_10_9_universal2.whl (10.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl (393 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Downloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, setuptools, protobuf, ml-dtypes, markdown, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.0\n",
      "    Uninstalling ml-dtypes-0.4.0:\n",
      "      Successfully uninstalled ml-dtypes-0.4.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.64.0 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 protobuf-4.25.3 setuptools-69.5.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 termcolor-2.4.0 werkzeug-3.0.3 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement '/kaggle/input/keras-lib-dataset/keras_nlp-0.9.2-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/keras-lib-dataset/keras_nlp-0.9.2-py3-none-any.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -q /kaggle/input/keras-lib-dataset/keras_nlp-0.9.2-py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: jax 0.4.28\n",
      "Uninstalling jax-0.4.28:\n",
      "  Would remove:\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/jax-0.4.28.dist-info/*\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/jax/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (0.4.28)\n",
      "Requirement already satisfied: jaxlib in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (0.4.28)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from jax) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from jax) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from jax) (1.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jax[cpu]\n",
      "  Using cached jax-0.4.28-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from jax[cpu]) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from jax[cpu]) (1.26.4)\n",
      "Collecting opt-einsum (from jax[cpu])\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting scipy>=1.9 (from jax[cpu])\n",
      "  Using cached scipy-1.13.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting jaxlib==0.4.28 (from jax[cpu])\n",
      "  Using cached jaxlib-0.4.28-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Using cached jaxlib-0.4.28-cp312-cp312-macosx_11_0_arm64.whl (64.1 MB)\n",
      "Using cached scipy-1.13.0-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "Using cached jax-0.4.28-py3-none-any.whl (1.9 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: scipy, opt-einsum, jaxlib, jax\n",
      "Successfully installed jax-0.4.28 jaxlib-0.4.28 opt-einsum-3.3.0 scipy-1.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"jax[cpu]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_nlp in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (0.11.1)\n",
      "Requirement already satisfied: keras-core in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (24.0)\n",
      "Requirement already satisfied: regex in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (2024.5.15)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras_nlp) (0.2.5)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kagglehub->keras_nlp) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kagglehub->keras_nlp) (4.66.4)\n",
      "Requirement already satisfied: namex in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras-core->keras_nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from keras-core->keras_nlp) (3.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from rich->keras_nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from rich->keras_nlp) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests->kagglehub->keras_nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests->kagglehub->keras_nlp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests->kagglehub->keras_nlp) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests->kagglehub->keras_nlp) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Using cached plotly-5.22.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly)\n",
      "  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from plotly) (24.0)\n",
      "Using cached plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
      "Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.22.0 tenacity-8.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.14.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kaggle) (4.66.4)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kaggle) (2.2.1)\n",
      "Requirement already satisfied: bleach in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages (from requests->kaggle) (3.7)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.14-py3-none-any.whl size=105119 sha256=929da7d9051b513ad8c1a24cad8aede91ee7dafe5365acab1d025756d2eda4e9\n",
      "  Stored in directory: /Users/jameskim/Library/Caches/pip/wheels/89/bc/52/0d140fc172783e1e72e3af4dfc13015dc154527fafb95e65cd\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.14 python-slugify-8.0.4 text-unidecode-1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/jameskim/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\" # avoid memory fragmentation on JAX backend.\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import kagglehub\n",
    "import kaggle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    dataset_path = \"/AI Math Olympiad Prize\"\n",
    "    preset = \"gemma_1.1_instruct_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training\n",
    "    epochs = 1 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility \n",
    "Sets value for random seed to produce similar result in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "No training data is provided in this competition; in other words, we can use any openly available datasets for this competition. In this notebook, we will use a modified **Math** dataset which I have compiled to have a `Question-Solution-Answer` format.\n",
    "\n",
    "**Data Format:**\n",
    "\n",
    "These datasets include:\n",
    "- `Question`: The math problem in LaTeX format.\n",
    "- `Solution`: Step-by-step solution to this problem.\n",
    "- `Answer`: Final answer of the solution which will be the ground truth for this competition.\n",
    "- `Level`: Difficulty of the problem.\n",
    "- `Type`: The category of the problem.\n",
    "\n",
    "> This dataset comes with its own train test split. However, we will merge them both and use them for fine-tuning. You are welcome to use them for trainining and validation separately. Also to reduce the training time we will only be training on the first`1000` samples. You are welcome to train on the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Level</th>\n",
       "      <th>Type</th>\n",
       "      <th>Solution</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My school's math club has 6 boys and 8 girls. ...</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>With no restrictions, we are merely picking 6 ...</td>\n",
       "      <td>3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many 4-letter words with at least one cons...</td>\n",
       "      <td>Level 3</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>First we count the number of all 4-letter word...</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let $p$ be the probability that, in the proces...</td>\n",
       "      <td>Level 5</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>Think of the problem as a sequence of H's and ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many four-digit numbers greater than 2999 ...</td>\n",
       "      <td>Level 4</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>For the first digit, there are seven choices (...</td>\n",
       "      <td>4970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have 5 marbles numbered 1 through 5 in a bag...</td>\n",
       "      <td>Level 3</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>There are $\\binom{5}{2} = 10$ different pairs ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The number of increasing sequences of positive...</td>\n",
       "      <td>Level 5</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>The numbers $a_i - i$ are ten not-necessarily ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Find the sum of all integers $k$ such that $\\b...</td>\n",
       "      <td>Level 4</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>By Pascal's Identity, we have $\\binom{23}{4} +...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Find the number of ordered pairs of positive i...</td>\n",
       "      <td>Level 5</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>There are $\\left\\lfloor\\frac{999}{10}\\right\\rf...</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Determine the number of ways to arrange the le...</td>\n",
       "      <td>Level 3</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>First we count the arrangements if all the let...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A mathematical organization is producing a set...</td>\n",
       "      <td>Level 5</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>If a sequence contains no more than one 0, the...</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question    Level  \\\n",
       "0  My school's math club has 6 boys and 8 girls. ...  Level 2   \n",
       "1  How many 4-letter words with at least one cons...  Level 3   \n",
       "2  Let $p$ be the probability that, in the proces...  Level 5   \n",
       "3  How many four-digit numbers greater than 2999 ...  Level 4   \n",
       "4  I have 5 marbles numbered 1 through 5 in a bag...  Level 3   \n",
       "5  The number of increasing sequences of positive...  Level 5   \n",
       "6  Find the sum of all integers $k$ such that $\\b...  Level 4   \n",
       "7  Find the number of ordered pairs of positive i...  Level 5   \n",
       "8  Determine the number of ways to arrange the le...  Level 3   \n",
       "9  A mathematical organization is producing a set...  Level 5   \n",
       "\n",
       "                       Type  \\\n",
       "0  counting_and_probability   \n",
       "1  counting_and_probability   \n",
       "2  counting_and_probability   \n",
       "3  counting_and_probability   \n",
       "4  counting_and_probability   \n",
       "5  counting_and_probability   \n",
       "6  counting_and_probability   \n",
       "7  counting_and_probability   \n",
       "8  counting_and_probability   \n",
       "9  counting_and_probability   \n",
       "\n",
       "                                            Solution Answer  \n",
       "0  With no restrictions, we are merely picking 6 ...   3003  \n",
       "1  First we count the number of all 4-letter word...    609  \n",
       "2  Think of the problem as a sequence of H's and ...     37  \n",
       "3  For the first digit, there are seven choices (...   4970  \n",
       "4  There are $\\binom{5}{2} = 10$ different pairs ...      6  \n",
       "5  The numbers $a_i - i$ are ten not-necessarily ...      8  \n",
       "6  By Pascal's Identity, we have $\\binom{23}{4} +...     24  \n",
       "7  There are $\\left\\lfloor\\frac{999}{10}\\right\\rf...    738  \n",
       "8  First we count the arrangements if all the let...     90  \n",
       "9  If a sequence contains no more than one 0, the...    372  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"traincompiled.csv\")\n",
    "df2 = pd.read_csv(\"testcompiled.csv\")\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "#df = df[:1000] # take first 1000 samples\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data\n",
    "\n",
    "The Math dataset contains various problems, but not all of them are suitable for this competition. More specifically, this competition requires a `non-negative integer` answer, while the Math dataset includes problems with different types of answers such as integers, floats, fractions, matrices, etc. In this notebook, we will only use those problems whose answers are non-negative integers and filter out the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Level</th>\n",
       "      <th>Type</th>\n",
       "      <th>Solution</th>\n",
       "      <th>Answer</th>\n",
       "      <th>is_integer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My school's math club has 6 boys and 8 girls. ...</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>With no restrictions, we are merely picking 6 ...</td>\n",
       "      <td>3003</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many 4-letter words with at least one cons...</td>\n",
       "      <td>Level 3</td>\n",
       "      <td>counting_and_probability</td>\n",
       "      <td>First we count the number of all 4-letter word...</td>\n",
       "      <td>609</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question    Level  \\\n",
       "0  My school's math club has 6 boys and 8 girls. ...  Level 2   \n",
       "1  How many 4-letter words with at least one cons...  Level 3   \n",
       "\n",
       "                       Type  \\\n",
       "0  counting_and_probability   \n",
       "1  counting_and_probability   \n",
       "\n",
       "                                            Solution Answer  is_integer  \n",
       "0  With no restrictions, we are merely picking 6 ...   3003        True  \n",
       "1  First we count the number of all 4-letter word...    609        True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_integer(text):\n",
    "    try:\n",
    "        if int(text) >= 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "df[\"is_integer\"] = df.Answer.map(is_integer)\n",
    "df = df[df.is_integer].reset_index(drop=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "We will be using below simple prompt template we'll use to create problem-solution-answer trio to feed the model. This template will help the model to follow instruction and respond accurately. You can explore more advanced prompt templates for better results. \n",
    "\n",
    "```\n",
    "Role:\n",
    "You are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n",
    "\n",
    "Instruction:\n",
    "1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n",
    "2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n",
    "3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n",
    "\n",
    "Problem:\n",
    "...\n",
    "\n",
    "Solution:\n",
    "...\n",
    "\n",
    "Answer:\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Role:\\nYou are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\\n\\nInstruction:\n",
    "1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n",
    "2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n",
    "3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\\n\\nProblem:\\n{problem}\\n\\nSolution:\\n{solution}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164618d4e06e4bbe94c79ce5f7b23b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"prompt\"] = df.progress_apply(lambda row: template.format(problem=row.Question,\n",
    "                                                             solution=f\"{row.Solution}\\n\\nAnswer:\\n{row.Answer}\"),\n",
    "                                                             axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample prompt. As the answers in our dataset are curated with **markdown** format, we will render the sample using `Markdown()` to properly visualize the formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Role\", \"Instruction\", \"Problem\", \"Solution\", \"Answer\"],\n",
    "                           [\"blue\", \"yellow\", \"red\", \"cyan\", \"green\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Role:</font>**\n",
       "You are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='yellow'>Instruction:</font>**\n",
       "1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n",
       "2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n",
       "3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Problem:</font>**\n",
       "Each block on the grid shown in the Figure is 1 unit by 1 unit.  Suppose we wish to walk from $A$ to $B$ via a 7 unit path, but we have to stay on the grid -- no cutting across blocks.  How many different paths can we take?[asy]size(3cm,3cm);int w=5;int h=4;int i;for (i=0; i<h; ++i){draw((0,i) -- (w-1,i));}for (i=0; i<w; ++i){draw((i, 0)--(i,h-1));}label(\"B\", (w-1,h-1), NE);label(\"A\", (0,0), SW);[/asy]\n",
       "\n",
       "\n",
       "\n",
       "**<font color='cyan'>Solution:</font>**\n",
       "We know that we must take a 7 unit path.  If we look at the grid a little more carefully, we can see that our path must consist of 4 steps to the right and 3 steps up, and we can take those steps in any order.  So in order to specify a path, we must choose 3 of our 7 steps to be `up' (and the other 4 steps will thus be `right').  Hence the number of paths is $$ \\binom{7}{3} = \\frac{7 \\times 6 \\times 5}{3 \\times 2 \\times 1} = \\boxed{35}. $$\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "35"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a random sample\n",
    "sample = data[12]\n",
    "\n",
    "# Give colors to Instruction, Response and Category\n",
    "sample = colorize_text(sample)\n",
    "\n",
    "# Show sample in markdown\n",
    "display(Markdown(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Role:</font>**\n",
       "You are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='yellow'>Instruction:</font>**\n",
       "1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n",
       "2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n",
       "3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Problem:</font>**\n",
       "Let $A$ equal the number of four digit odd numbers.  Let $B$ equal the number of four digit multiples of 5.  Find $A+B$.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='cyan'>Solution:</font>**\n",
       "For an odd number, there are 5 choices for the units digit, coming from the set $\\{1,3,5,7,9\\}$.  There will be 10 choices for the tens digit, 10 choices for the hundreds digit, and 9 choices for the thousands digit, which can not be zero.  This is a total of: $$9\\times10\\times10\\times5=4500\\text{ four digit odd numbers}$$Multiples of 5 must end in 0 or 5.  So, there are two possibilities for the units digit, and the same number of possibilities for remaining digits.  This gives: $$9\\times10\\times10\\times2=1800\\text{ four digit multiples of 5}$$Therefore, $A+B=4500+1800=\\boxed{6300}$.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "6300"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a random sample\n",
    "sample = data[32]\n",
    "\n",
    "# Give colors to Instruction, Response and Category\n",
    "sample = colorize_text(sample)\n",
    "\n",
    "# Show sample in markdown\n",
    "display(Markdown(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.ibb.co/Bqg9w3g/Gemma-Logo-no-background.png\" width=\"300\"></div>\n",
    "\n",
    "**Gemma** is a collection of advanced open LLMs developed by **Google DeepMind** and other **Google teams**, derived from the same research and technology behind the **Gemini** models. They can be integrated into applications and run on various platforms including mobile devices and hosted services. Developers can customize Gemma models using tuning techniques to enhance their performance for specific tasks, offering more targeted and efficient generative AI solutions beyond text generation.\n",
    "\n",
    "Gemma models are available in several sizes so we can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them.\n",
    "\n",
    "| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n",
    "|-----------------|-------------------|------------------------------------|------------------------|\n",
    "| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n",
    "| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_1.1_instruct_2b_en` |\n",
    "| 2B              | Pretrained        | Code Completion in Mobile Device   | `code_gemma_2b_en` |\n",
    "| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n",
    "| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_1.1_instruct_7b_en` |\n",
    "| 7B              | Instruction tuned | Code Completion in Desktop computers| `code_gemma_7b_en` |\n",
    "\n",
    "In this notebook, we will utilize the `Gemma 1.1 2b-it` model from KerasNLP's pretrained models to solve the math olympiad questions. We are using the \"Instruction tuned\" model instead of the \"Pretrained\" one because it is easier for the model to fine-tune on the prepared dataset. \n",
    "\n",
    "To explore other available models, you can simply adjust the `preset` value in the `CFG` (config). You can find a list of other pretrained models on the [KerasNLP website](https://keras.io/api/keras_nlp/models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma Causal LM\n",
    "\n",
    "The code below will build an end-to-end Gemma model for causal language modeling (hence the name `GemmaCausalLM`). A causal language model (LM) predicts the next token based on previous tokens. This task setup can be used to train the model unsupervised on plain text input or to autoregressively generate plain text similar to the data used for training. This task can be used for pre-training or fine-tuning a Gemma model simply by calling `fit()`.\n",
    "\n",
    "This model has a `generate()` method, which generates text based on a prompt. The generation strategy used is controlled by an additional sampler argument on `compile()`. You can recompile the model with different `keras_nlp.samplers` objects to control the generation. By default, `\"greedy\"` sampling will be used.\n",
    "\n",
    "> The `from_preset` method instantiates the model from a preset architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kaggle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mkaggle\u001b[49m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mauthenticate()\n\u001b[1;32m      3\u001b[0m kagglehub\u001b[38;5;241m.\u001b[39mlogin()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kaggle' is not defined"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "kaggle.api.authenticate()\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0a8fdc158c400a82066a5588d21ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/3/download...\n"
     ]
    },
    {
     "ename": "KaggleApiHTTPError",
     "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/3\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/kagglehub/exceptions.py:58\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/3/download",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKaggleApiHTTPError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kagglehub\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m----> 4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras/gemma/keras/gemma_1.1_instruct_2b_en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to model files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/kagglehub/models.py:25\u001b[0m, in \u001b[0;36mmodel_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download model files.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    A string representing the path to the requested model files.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_model_handle(handle)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/kagglehub/registry.py:23\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/kagglehub/http_resolver.py:53\u001b[0m, in \u001b[0;36mModelHttpResolver.__call__\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m     50\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(archive_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# First, we download the archive.\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Create the directory to extract the archive to.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/kagglehub/clients.py:136\u001b[0m, in \u001b[0;36mKaggleApiV1Client.download_file\u001b[0;34m(self, path, out_file, resource_handle)\u001b[0m\n\u001b[1;32m    128\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    130\u001b[0m     url,\n\u001b[1;32m    131\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_user_agent()},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[1;32m    135\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mkaggle_api_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    138\u001b[0m     size_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.0/libexec/lib/python3.12/site-packages/kagglehub/exceptions.py:80\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     72\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource not found at URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKaggleApiHTTPError\u001b[0m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/3\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
     ]
    }
   ],
   "source": [
    "path = kagglehub.model_download(\"keras/gemma/keras/gemma_1.1_instruct_2b_en\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/3/download/metadata.json...\n",
      "100%|██████████████████████████████████████████| 142/142 [00:00<00:00, 67.1kB/s]\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/3/download/task.json...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/3/download/config.json...\n",
      "100%|███████████████████████████████████████████| 554/554 [00:00<00:00, 311kB/s]\n"
     ]
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma LM Preprocessor\n",
    "\n",
    "An important part of the Gemma model is the **Preprocessor** layer, which under the hood uses **Tokenizer**.\n",
    "\n",
    "**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n",
    "\n",
    "**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n",
    "\n",
    "Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n",
    "- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n",
    "- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:18:09.095582Z",
     "iopub.status.busy": "2024-04-19T14:18:09.095219Z",
     "iopub.status.idle": "2024-04-19T14:18:09.443047Z",
     "shell.execute_reply": "2024-04-19T14:18:09.442022Z",
     "shell.execute_reply.started": "2024-04-19T14:18:09.095551Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing layer will take in batches of strings, and return outputs in a `(x, y, sample_weight)` format, where the `y` label is the next token id in the `x` sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is `(num_samples, sequence_length)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:18:09.444834Z",
     "iopub.status.busy": "2024-04-19T14:18:09.444468Z",
     "iopub.status.idle": "2024-04-19T14:18:09.450519Z",
     "shell.execute_reply": "2024-04-19T14:18:09.449607Z",
     "shell.execute_reply.started": "2024-04-19T14:18:09.444801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the shape of each processed output\n",
    "for k, v in x.items():\n",
    "    print(k, \":\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference before Fine-Tuning\n",
    "\n",
    "Before we do fine-tuning, let's see how Gemma model responds with some prepared prompts.\n",
    "\n",
    "> As this model is not yet fine-tuned for instruction, you will notice that the model's responses are inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:18:09.452566Z",
     "iopub.status.busy": "2024-04-19T14:18:09.451781Z",
     "iopub.status.idle": "2024-04-19T14:18:26.254776Z",
     "shell.execute_reply": "2024-04-19T14:18:26.25356Z",
     "shell.execute_reply.started": "2024-04-19T14:18:09.452534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take one sample\n",
    "row = df.iloc[12]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = template.format(\n",
    "    problem=row.problem,\n",
    "    solution=\"\",\n",
    ")\n",
    "\n",
    "# Infer\n",
    "output = gemma_lm.generate(prompt, max_length=1024)\n",
    "\n",
    "# Colorize\n",
    "output = colorize_text(output)\n",
    "\n",
    "# Display in markdown\n",
    "display(Markdown(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:18:26.256488Z",
     "iopub.status.busy": "2024-04-19T14:18:26.256154Z",
     "iopub.status.idle": "2024-04-19T14:18:27.901428Z",
     "shell.execute_reply": "2024-04-19T14:18:27.900201Z",
     "shell.execute_reply.started": "2024-04-19T14:18:26.25646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take one sample\n",
    "row = df.iloc[32]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = template.format(\n",
    "    problem=row.problem,\n",
    "    solution=\"\"\n",
    ")\n",
    "\n",
    "# Infer\n",
    "output = gemma_lm.generate(prompt, max_length=1024)\n",
    "\n",
    "# Colorize\n",
    "output = colorize_text(output)\n",
    "\n",
    "# Display in markdown\n",
    "display(Markdown(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with LoRA\n",
    "\n",
    "To get better responses from the model, we will fine-tune the model with Low Rank Adaptation (LoRA).\n",
    "\n",
    "**What exactly is LoRA?**\n",
    "\n",
    "LoRA is a method used to fine-tune large language models (LLMs) in an efficient way. It involves freezing the weights of the LLM and injecting trainable rank-decomposition matrices.\n",
    "\n",
    "Imagine in an LLM, we have a pre-trained dense layer, represented by a $d \\times d$ weight matrix, denoted as $W_0$. We then initialize two additional dense layers, labeled as $A$ and $B$, with shapes $d \\times r$ and $r \\times d$, respectively. Here, $r$ denotes the rank, which is typically **much smaller than** $d$. Prior to LoRA, the model's output was computed using the equation $output = W_0 \\cdot x + b_0$, where $x$ represents the input and $b_0$ denotes the bias term associated with the original dense layer, which remains frozen. After applying LoRA, the equation becomes $output = (W_0 \\cdot x + b_0) + (B \\cdot A \\cdot x)$, where $A$ and $B$ denote the trainable rank-decomposition matrices that have been introduced.\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/DWsbhLg/LoRA.png\" width=\"300\"><br/>\n",
    "Credit: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a> Paper</center>\n",
    "\n",
    "\n",
    "In the LoRA paper, $A$ is initialized with $\\mathcal{N} (0, \\sigma^2)$ and $B$ with $0$, where $\\mathcal{N}$ denotes the normal distribution, and $\\sigma^2$ is the variance.\n",
    "\n",
    "**Why does LoRA save memory?**\n",
    "\n",
    "Even though we're adding more layers to the model with LoRA, it actually helps save memory. This is because the smaller layers (A and B) have fewer parameters to learn compared to the big model and fewer trainable parameters mean fewer optimizer variables to store. So, even though the overall model might seem bigger, it's actually more efficient in terms of memory usage. \n",
    "\n",
    "> This notebook uses a LoRA rank of `4`. A higher rank means more detailed changes are possible, but also means more trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:18:27.903949Z",
     "iopub.status.busy": "2024-04-19T14:18:27.902999Z",
     "iopub.status.idle": "2024-04-19T14:18:28.380656Z",
     "shell.execute_reply": "2024-04-19T14:18:28.379767Z",
     "shell.execute_reply.started": "2024-04-19T14:18:27.90391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$1.3$ millions after enabling LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:18:28.382975Z",
     "iopub.status.busy": "2024-04-19T14:18:28.382538Z",
     "iopub.status.idle": "2024-04-19T14:27:05.615248Z",
     "shell.execute_reply": "2024-04-19T14:27:05.614308Z",
     "shell.execute_reply.started": "2024-04-19T14:18:28.382941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = CFG.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference after fine-tuning\n",
    "\n",
    "Let's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:40:17.963709Z",
     "iopub.status.busy": "2024-04-19T14:40:17.963335Z",
     "iopub.status.idle": "2024-04-19T14:40:22.677954Z",
     "shell.execute_reply": "2024-04-19T14:40:22.677051Z",
     "shell.execute_reply.started": "2024-04-19T14:40:17.963679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take one sample\n",
    "row = df.iloc[12]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = template.format(\n",
    "    problem=row.problem,\n",
    "    solution=\"\"\n",
    ")\n",
    "\n",
    "# Infer\n",
    "output = gemma_lm.generate(prompt, max_length=1024)\n",
    "\n",
    "# Colorize\n",
    "output = colorize_text(output)\n",
    "\n",
    "# Display in markdown\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:38:50.004424Z",
     "iopub.status.busy": "2024-04-19T14:38:50.003518Z",
     "iopub.status.idle": "2024-04-19T14:38:53.287786Z",
     "shell.execute_reply": "2024-04-19T14:38:53.286864Z",
     "shell.execute_reply.started": "2024-04-19T14:38:50.004389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take one sample\n",
    "row = df.iloc[32]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = template.format(\n",
    "    problem=row.problem,\n",
    "    solution=\"\"\n",
    ")\n",
    "\n",
    "# Infer\n",
    "output = gemma_lm.generate(prompt, max_length=1024)\n",
    "\n",
    "# Colorize\n",
    "output = colorize_text(output)\n",
    "\n",
    "# Display in markdown\n",
    "display(Markdown(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO Data\n",
    "\n",
    "So far we have inferred our model on **Math** dataset but now let's see how our model perform on AIMO (competition) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:27:25.369306Z",
     "iopub.status.busy": "2024-04-19T14:27:25.369058Z",
     "iopub.status.idle": "2024-04-19T14:27:25.377883Z",
     "shell.execute_reply": "2024-04-19T14:27:25.376902Z",
     "shell.execute_reply.started": "2024-04-19T14:27:25.369284Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract answer from model response\n",
    "def get_answer(text):\n",
    "    try:\n",
    "        answer = re.search(r'Answer:\\s*([\\s\\S]+)', text).group(1).strip()\n",
    "        answer = answer.replace(\",\",\"\")\n",
    "        if is_integer(answer):\n",
    "            return int(answer)%1000\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def infer(df):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        # Generate Prompt using template\n",
    "        prompt = template.format(\n",
    "            problem=row.problem,\n",
    "            solution=\"\"\n",
    "        )\n",
    "\n",
    "        # Infer\n",
    "        output = gemma_lm.generate(prompt, max_length=1024)\n",
    "        pred = get_answer(output)\n",
    "\n",
    "        # Store predictions\n",
    "        preds.append([row.id, pred])\n",
    "        if \"answer\" in row:\n",
    "            preds[-1] += [row.answer]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on AIMO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:27:25.379258Z",
     "iopub.status.busy": "2024-04-19T14:27:25.378995Z",
     "iopub.status.idle": "2024-04-19T14:28:06.239544Z",
     "shell.execute_reply": "2024-04-19T14:28:06.238644Z",
     "shell.execute_reply.started": "2024-04-19T14:27:25.379236Z"
    }
   },
   "outputs": [],
   "source": [
    "aimo_df = pd.read_csv(f\"{CFG.dataset_path}/train.csv\")\n",
    "train_preds = infer(aimo_df)\n",
    "train_pred_df = pd.DataFrame(train_preds, columns=[\"id\", \"prediction\", \"answer\"])\n",
    "train_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:28:06.241057Z",
     "iopub.status.busy": "2024-04-19T14:28:06.240768Z",
     "iopub.status.idle": "2024-04-19T14:28:09.60068Z",
     "shell.execute_reply": "2024-04-19T14:28:09.599788Z",
     "shell.execute_reply.started": "2024-04-19T14:28:06.241033Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f\"{CFG.dataset_path}/test.csv\")\n",
    "test_preds = infer(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission File\n",
    "\n",
    "While preparing the submission file, we must keep in mind that, the answer must be between `0-999`. This can easily handled by using `remainder (%)` operation. For this notebook, this step is already applied in the inference stage while extracting `answer` from `solution`. So, we don't need to separately apply it heer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:28:09.602515Z",
     "iopub.status.busy": "2024-04-19T14:28:09.602126Z",
     "iopub.status.idle": "2024-04-19T14:28:09.617016Z",
     "shell.execute_reply": "2024-04-19T14:28:09.615904Z",
     "shell.execute_reply.started": "2024-04-19T14:28:09.602482Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(test_preds, columns=[\"id\", \"answer\"])\n",
    "sub_df.to_csv(\"submission.csv\",index=False,header=True)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We can see that after fine-tuning, the model is following instructions more accurately. However, it may still struggle to solve problems accurately, which can be attributed to its small size. Nevertheless, there is ample room for improvement. Here are some tips to enhance performance:\n",
    "\n",
    "- Train on the full data instead of first `1000` samples.\n",
    "- Try using the non-instruction-tuned version of Gemma.\n",
    "- Increase the `sequence_length`.\n",
    "- Experiment with advanced prompt engineering techniques.\n",
    "- Implement augmentation to increase the number of samples.\n",
    "- Utilize a learning rate scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)\n",
    "* [Parameter-efficient fine-tuning of GPT-2 with LoRA](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)\n",
    "* [Gemma - KerasNLP](https://keras.io/api/keras_nlp/models/gemma/)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4717827,
     "sourceId": 8009768,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4766079,
     "sourceId": 8076106,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 22009,
     "sourceId": 27825,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
