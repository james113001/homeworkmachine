


#pip install transformers evaluate accelerate datasets scikit-learn torch tf-keras openpyxl xlsxwriter pylatexenc


import pandas as pd
# from sklearn.preprocessing import LabelEncoder


# Load your dataset
train_df = pd.read_csv('../resources/ALLtraincompiled.csv')
test_df = pd.read_csv('../resources/ALLtestcompiled.csv')

train_df = train_df[['Question','Type']]
test_df = test_df[['Question','Type']]

# Split the dataset into training and test sets
# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Define the label mapping
label_mapping = {'counting_and_probability': 0, 'intermediate_algebra': 1, 'number_theory': 2, 'precalculus':3, 
                'prealgebra':4, 'geometry':5, 'algebra':6}
train_df['Type'] = train_df['Type'].map(label_mapping)
test_df['Type'] = test_df['Type'].map(label_mapping)


# Map the labels to integers
train_df['labels'] = train_df['Type']
test_df['labels'] = test_df['Type']

# # Encode the labels
# label_encoder = LabelEncoder()
# train_df['Type'] = label_encoder.fit_transform(train_df['Type'])
# test_df['Type'] = label_encoder.transform(test_df['Type'])


train_df['Type'].value_counts()


test_df['Type'].value_counts()


train_df.head(20)


import re

def remove_commas_from_latex_numbers(text):
    # Regular expression to match numbers with commas
    #pattern = r'(\$[0-9]\d{0,2}(,\!?\d{3})*(\.\d+)?\$)'
    pattern = r'(\$[0-9]\d{0,2}(,\\?\!?\\?\d{3})*(\.\d+)?\$)'

    # Function to remove commas from matched numbers
    def remove_commas(match):
        return match.group().replace(',', '').replace(r'\!', '')

    # Replace commas in matched numbers
    result = re.sub(pattern, remove_commas, text)
    return result

# Example input string
input_text = r"""
Cedric has deposited $\$12,\!000$ into an account that pays $5\%$ interest compounded annually.

Daniel has deposited $\$12,\!000$ into an account that pays $7\%$ simple annual interest.

In $15$ years Cedric and Daniel compare their respective balances. To the nearest dollar, what is the positive difference between their balances?
"""

# Apply the function to the input string
output_text = remove_commas_from_latex_numbers(input_text)

# Print the output string
print(output_text)



# pip install pydetex


import re

def handle_specific_commands(latex):

    command_pattern = r'\\([a-zA-Z]+)(?:\{(.*?)\})?(?:\{(.*?)\})?'
    
    # Define a dictionary to map LaTeX commands to their plain text equivalents
    command_mapping = {
        '\\frac': lambda args: f"{args[0]} divided by {args[1]}",
        '\\binom': lambda args: f"{args[0]} choose {args[1]}",
        '\\dbinom': lambda args: f"{args[0]} double choose {args[1]}"
        # Add more mappings as needed
    }
    
    # Function to handle LaTeX commands and their arguments
    def handle_command(match):
        command = match.group(1)
        arguments = match.group(2)[1:-1] if match.group(2) else ''  # Remove enclosing braces if present
        args = arguments.split('{')[1:] if arguments else []  # Split arguments if present
        args = [arg.split('}')[0] for arg in args]  # Remove closing braces
        if command in command_mapping:
            return command_mapping[command](args)
        else:
            return match.group(0)  # Return unchanged if command is not mapped
    
    # Replace LaTeX commands with their plain text equivalents
    plain_text = re.sub(command_pattern, handle_command, latex)
    
    return plain_text


import re
from pylatexenc.latex2text import LatexNodes2Text, MacroTextSpec
from datasets import Dataset, DatasetDict


# class CustomLatexNodes2Text(LatexNodes2Text):
#     def __init__(self, latex_context=None):
#         if latex_context is None:
#             self.latex_context = self.get_default_latex_context_db()
#         super().__init__(latex_context=self.latex_context)

#     def get_custom_context(self):
#         context = self.latex_context
#         # Add custom macros here
#         context.add_context_category('custom-macros', [
#             MacroTextSpec('frac', args_parser='{}{}', macro_postproc=self.convert_frac),
#             MacroTextSpec('cdot', macro_postproc=lambda n, **kw: '路'),

#             # We can add more custom macros as needed but too many can lead to internal clashes with LatexNodes2Text internal translation
#         ])
#         return context

#     def convert_frac(self, n, **kwargs):
#         numerator = self.nodelist_to_text(n.nodeargd.argnlist[0])
#         denominator = self.nodelist_to_text(n.nodeargd.argnlist[1])
#         return f"({numerator} / {denominator})"

#LatexNodes2Text doesn't work for binomial, combinations, triple dots?
def custom_latex_to_text(latex):
    latex = re.sub(r'\\choose', ' choose', latex)
    latex = re.sub(r'\\binom\{(.*?)\}\{(.*?)\}', r'\1 choose \2', latex)
    latex = re.sub(r'\\dbinom\{(.*?)\}\{(.*?)\}', r'\1 choose \2', latex)
    latex = re.sub(r'\\cdots', '...', latex)
    latex = re.sub(r'\\cdot', '路', latex)                                    # multiplication
    latex = re.sub(r'\\spadesuit', 'spade' , latex)
    latex = re.sub(r'\\clubsuit', 'club', latex)
    latex = re.sub(r'\\diamondsuit', 'diamond' , latex)
    latex = re.sub(r'\\heartsuit', 'heart' , latex)
    latex = re.sub(r'\\dotsm', '路 ... 路' , latex)
    latex = re.sub(r'\\left\\lfloor\s*(.*?)\s*\\right\\rfloor', r'floor(\1)', latex)
    #latex = re.sub(r'\\frac\{(.*?)\}\{(.*?)\}', r'\1 / \2', latex)
    latex = re.sub(r'\\tfrac\{(.*?)\}\{(.*?)\}', r'\1 divided by \2', latex)
    latex = re.sub(r'\\dfrac\{(.*?)\}\{(.*?)\}', r'\1 divided by \2', latex)
    #latex = re.sub(r'\\sum_\{(.*?)\}\^\{(.*?)\}', r'sum from \1 to \2 of', latex)
    #latex = re.sub(r'\\sum_\s*\{(.*?)\}\s*\^\s*\{(.*?)\}', r'sum from \1 to \2 of', latex)
    #latex = re.sub(r'\\sum_\s*\{(.*?)\}\s*\^\s*\{(.*?)\}', r'sum from \1 to \2 of', latex)
    #latex = re.sub(r'\\prod_\{(.*?)\}\^\{(.*?)\}', r'product from \1 to \2 of', latex)
    #latex = re.sub(r'\\sum_\s*\{(.*?)\}\s*\^\s*\{(.*?)\}', r'sum from \1 to \2 of', latex) # inc ase that we parsed in \\sum_or also \sum_
    #latex = re.sub(r'\\sum_\{(.*?)\}\s*\^\s*\{(.*?)\}', r'sum from \1 to \2 of', latex)
    latex = re.sub(r'\\sum_\s*\{(.*?)\}\s*\^\s*\{(.*?)\}', lambda m: 'sum from ' + m.group(1) + ' to ' + ('infinity' if m.group(2) == '\\infty' else m.group(2)) + ' of', latex) # lambda used to select first parameter as start of sum and second parameter as end of sum and if secomnd param is infty then chosoe text "infinity"
    latex = re.sub(r'\\prod_\s*\{(.*?)\}\s*\^\s*\{(.*?)\}', r'product from \1 to \2 of', latex) # same case as above but \\prod_ or also \prod_
    latex = re.sub(r'\\circ', 'composed with', latex)
    latex = re.sub(r'\\lim\{(.*?)\}', r'lim as \1', latex)
    latex = re.sub(r'\\sin\{(.*?)\}', r'sin(\1)', latex)
    latex = re.sub(r'\\cos\{(.*?)\}', r'cos(\1)', latex)
    latex = re.sub(r'\\log\{(.*?)\}', r'log(\1)', latex)

    return latex

def remove_comma_from_latex(latex_list):
    # Regular expression to match numbers with commas
    #pattern = r'\$([1-9][0-9]{3,}|[1-9][0-9]{5,})'  
    pattern = r'(\$[0-9]\d{0,2}(,\\?\!?\\?\d{3})*(\.\d+)?\$)'
    
    # Function to remove commas from matched numbers
    def remove_comma(match):
        return match.group().replace(',', '')
    
    # Replace commas in numbers >= 1000 for each element in the list
    modified_latex_list = []
    for latex in latex_list:
        modified_latex = re.sub(pattern, remove_comma, latex)
        modified_latex_list.append(modified_latex)
        
    return modified_latex_list
# Test the function with an example
# latex_value = r"$12,\!000$"


# Function to extract LaTeX phrases
def extract_latex(text):
    # Pattern to match $...$, $$...$$, or also \[...\] (all different types of LaTeX for different displaying purposes but our model doesn't care about that)
    # pattern = r'\$(.*?)\$|\$\$(.*?)\$\$|\\\[(.*?)\\\]'
    # matches = re.findall(pattern, text)
    # latex_phrases = [match for groups in matches for match in groups if match] # ugly comprehension, goes through each tuple in the nested tuple the findall gives us and returns the non-empty matches
    # return latex_phrases

    pattern = r'(\$.*?\$)|(\$\$.*?\$\$)|(\\\[.*?\\\])'
    matches = re.findall(pattern, text)
    latex_phrases = [match[0] or match[1] or match[2] for match in matches] 
    return latex_phrases

    #expressions = re.findall(pattern, text, flags=re.DOTALL)
    #return [expr[0] if expr[0] else (expr[1] if expr[1] else expr[2]) for expr in expressions]
    #pattern = r'\$[^\$]*?\$|\$\$[^\$]*?\$\$'
    #return re.findall(pattern, text)

# Function to convert LaTeX to plain text using both LatexNodes2Text and custom 
def convert_latex_to_text(latex_phrases):
    custom_converted = [custom_latex_to_text(phrase) for phrase in latex_phrases]
    # # Convert remaining LaTeX to plain text using LatexNodes2Text
    #custom_converted = [handle_specific_commands(phrase) for phrase in latex_phrases]
    converter = LatexNodes2Text()
    return [converter.latex_to_text(phrase) for phrase in custom_converted]


def replace_latex_with_text(question, latex_phrases, text_phrases):
    for latex, text in zip(latex_phrases, text_phrases):
        question = question.replace(latex, text)
        #question = question.replace(latex, text)
    return question

# Extract LaTeX phrases and convert them
# train_df['Question'] = train_df['Question'].apply(remove_comma_from_latex)
train_df['Latex_Phrases'] = train_df['Question'].apply(extract_latex)
train_df['Math_Text'] = train_df['Latex_Phrases'].apply(convert_latex_to_text)
train_df['New_Question'] = train_df.apply(lambda row: replace_latex_with_text(row['Question'], row['Latex_Phrases'], row['Math_Text']), axis=1)

test_df['Latex_Phrases'] = test_df['Question'].apply(extract_latex)
test_df['Math_Text'] = test_df['Latex_Phrases'].apply(convert_latex_to_text)
test_df['New_Question'] = test_df.apply(lambda row: replace_latex_with_text(row['Question'], row['Latex_Phrases'], row['Math_Text']), axis=1)


# # Print the DataFrame
#print(train_df[['Question','Latex_Phrases','Math_Text','New_Question']])
# train_df.to_csv('../resources/train_questions_with_plain_text.csv', index=False)
# train_df.to_excel('../resources/train_questions_with_plain_text.xlsx', index=False, engine='xlsxwriter')

# test_df.to_csv('../resources/test_questions_with_plain_text.csv', index=False)


train_dataset


latex = ['\\[\\sum_{n = 1}^\\infty \\frac{2n - 1}{n(n + 1)(n + 2)}.\\]']
for phrase in latex:
    print(custom_latex_to_text(phrase))


train_df["Question"].iloc[157]

"Will we have questions with tables?"
>


train_df["Question"].iloc[203]

"Will we have questions with a draw functionality?"


train_df["Question"].iloc[1421]


train_df['Question'].iloc[714]


train_df['New_Question'].iloc[714]


train_df['Latex_Phrases'].iloc[551]


train_df["New_Question"].iloc[551]


train_df["New_Question"].iloc[80]


train_df["Question"].iloc[947]


train_df["New_Question"].iloc[947]


train_df['Latex_Phrases'].iloc[80]


train_df['Math_Text'].iloc[80]


train_df["Latex_Phrases"].iloc[1445]


train_df["Math_Text"].iloc[1445]


train_df["New_Question"].iloc[1445]





from latex2sympy2 import latex2sympy

train_df['sympy_column'] = train_df['latex_column'].apply(convert_to_sympy)
print(x)
# => "diff(x**(2), x)"


latexx= '$\$12,\!000$'
latex2sympy(latexx)








import pandas as pd
import re
from pylatexenc.latex2text import LatexNodes2Text, MacroTextSpec
from datasets import Dataset, DatasetDict


# Load your dataset
train_df = pd.read_csv('../resources/train_questions_with_plain_text.csv')
test_df = pd.read_csv('../resources/test_questions_with_plain_text.csv')

train_dataset = train_df[['labels', 'New_Question']]
test_dataset = test_df[['labels','New_Question']]


# Convert DataFrames to Hugging Face Dataset
train_dataset = Dataset.from_pandas(train_dataset)
test_dataset = Dataset.from_pandas(test_dataset)


from transformers import BertTokenizer
import numpy as np
import torch

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_data(examples):
    return tokenizer(examples['New_Question'], padding='max_length', truncation=True)


# train_dataset = train_dataset.map(tokenize_data, batched=True)
tokenized_train_dataset = train_dataset.map(tokenize_data, batched=True, num_proc=4)
tokenized_test_dataset = test_dataset.map(tokenize_data, batched=True, num_proc=4)

tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])


# import json

# # Utility function to check if an object is JSON serializable
# def is_serializable(obj):
#     try:
#         json.dumps(obj)
#         return True
#     except (TypeError, OverflowError):
#         return False

# # Function to check the serializability of dataset entries
# def check_serializability(dataset, column_name):
#     non_serializable_indices = []
#     for idx, entry in enumerate(dataset[column_name]):
#         if not is_serializable(entry):
#             non_serializable_indices.append(idx)
#             print(f"Non-serializable data found at index {idx}: {entry}")

#     if not non_serializable_indices:
#         print(f"All data in the {column_name} column is serializable.")
#     else:
#         print(f"Found non-serializable data at indices: {non_serializable_indices}")

# # Assuming tokenized_datasets is your DatasetDict
# # Check the 'input_ids' column for each split in tokenized_datasets
# for split in tokenized_datasets.keys():
#     print(f"Checking serializability for {split} split:")
#     check_serializability(tokenized_datasets[split], 'input_ids')






# latex_pattern = r'\$([^$]*)\$'

# # Tokenization function with LaTeX support
# def tokenize_function(examples):
#     tokenized_inputs = {'input_ids': [], 'attention_mask': [], 'labels': examples['labels']}
    
#     for problem_text in examples['New_Question']:
#         matches = re.finditer(latex_pattern, problem_text)
#         tokenized_problem = []
#         last_end = 0
#         for match in matches:
#             start, end = match.span()
#             tokenized_problem.extend(tokenizer.tokenize(problem_text[last_end:start]))
#             latex_expression = match.group(1)
#             tokenized_latex = tokenizer.tokenize(latex_expression)
#             tokenized_problem.extend(tokenized_latex)
#             last_end = end
#         tokenized_problem.extend(tokenizer.tokenize(problem_text[last_end:]))
#         encoded = tokenizer.encode_plus(tokenized_problem, padding='max_length', truncation=True)
#         tokenized_inputs['input_ids'].append(encoded['input_ids'])
#         tokenized_inputs['attention_mask'].append(encoded['attention_mask'])
    
#     return tokenized_inputs

# def filter_and_serialize(examples):
#     serialized_examples = {key: ensure_serializable(value) for key, value in examples.items()}
#     # Keep only the required columns for training
#     return {
#         'input_ids': serialized_examples['input_ids'],
#         'attention_mask': serialized_examples['attention_mask'],
#         'labels': serialized_examples['labels']
#     }
# # Tokenization function
# # Tokenization function with debugging
# # def tokenize_function(examples):
# #     tokenized_inputs = tokenizer(examples['Question'], padding='max_length', truncation=True)
# #     # print("Example:", examples['Question'])
# #     # print("Tokenized inputs:", tokenized_inputs)
# #     return tokenized_inputs

# # Tokenization function
# def tokenize_function(examples):
#     return tokenizer(
#         examples['New_Question'], 
#         padding='max_length', 
#         truncation=True, 
#         max_length=128, 
#         return_tensors='pt'
#     )


# # Apply tokenization
# train_dataset_tokenized = tokenized_datasets['train'].map(tokenize_function, batched=True)
# test_dataset_tokenized = tokenized_datasets['test'].map(tokenize_function, batched=True)


# # # Set format for PyTorch tensors
# # train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
# # test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# train_dataset_tokenized = train_dataset_tokenized.map(filter_and_serialize, batched=True)
# test_dataset_tokenized = test_dataset_tokenized.map(filter_and_serialize, batched=True)



# try:
#     tokenized_datasets['train'].to_json("train_dataset.json")
#     tokenized_datasets['test'].to_json("test_dataset.json")
#     print("Datasets are serializable.")
# except Exception as e:
#     print(f"Serialization error: {e}")


import torch
from torch.utils.data import DataLoader
from transformers import BertForSequenceClassification, AdamW, get_scheduler
from peft import LoraConfig, get_peft_model, TaskType
from tqdm import tqdm

# Load BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train_df['labels'].unique()))


# Apply LoRA to the BERT model
config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,  # Rank of the low-rank matrices
    lora_alpha=32,  # Scaling factor for the LoRA updates
    # target_modules=["query", "key", "value"],  # Apply LoRA to these layers
    lora_dropout=0.1,  # Dropout rate
)

model = get_peft_model(model, config)


train_dataloader = DataLoader(tokenized_train_dataset, batch_size=16, shuffle=True)
test_dataloader = DataLoader(tokenized_test_dataset, batch_size=16)

optimizer = AdamW(model.parameters(), betas=(0.9, 0.999), eps=1e-08)

num_training_steps = len(train_dataloader) * 3  # Assuming 3 epochs
lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)


device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
model.to(device)


def train_model(model, train_dataloader, optimizer, lr_scheduler, num_epochs=3):
    model.train()
    for epoch in range(num_epochs):
        for batch in tqdm(train_dataloader, desc=f"Training Epoch {epoch+1}"):
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
        print(f"Epoch {epoch+1} completed")

# Train the model
train_model(model, train_dataloader, optimizer, lr_scheduler)


def evaluate_model(model, dataloader):
    model.eval()
    predictions, true_labels = [], []
    for batch in tqdm(dataloader, desc="Evaluating"):
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)
        predictions.extend(preds.cpu().numpy())
        true_labels.extend(batch['labels'].cpu().numpy())
    return predictions, true_labels
    
# Evaluate the model
preds, labels = evaluate_model(model, test_dataloader)

# Calculate evaluation metrics
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

accuracy = accuracy_score(labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")


# Save the model and tokenizer
model.save_pretrained('../resources/fine-tuned-bert-lora')
tokenizer.save_pretrained('../resources/fine-tuned-bert-lora')

print("Model and tokenizer saved.")








# Load model directly
import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import DataLoader
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
from datasets import Dataset

import pandas as pd

tokenizer = AutoTokenizer.from_pretrained("lschlessinger/bert-finetuned-math-prob-classification")
model = AutoModelForSequenceClassification.from_pretrained("lschlessinger/bert-finetuned-math-prob-classification")

test_df = pd.read_csv('../resources/ALLtestcompiled.csv')
# Convert 'Type' column to categorical if not already
test_df['Type'] = test_df['Type'].astype('category')

# Map the labels to integers
test_df['label'] = test_df['Type'].cat.codes

test_df = test_df[['label','Question']]
# Convert to Hugging Face dataset
test_dataset = Dataset.from_pandas(test_df)

# Function to tokenize data with truncation
def tokenize_data(examples):
    return tokenizer(examples['Question'], padding='max_length', truncation=True, max_length=512)

# Tokenize the datasets
tokenized_test_dataset = test_dataset.map(tokenize_data, batched=True, num_proc=4)
tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# classifier = pipeline("text-classification", model="lschlessinger/bert-finetuned-math-prob-classification")
# classifier(tokenized_datasets['test']['New_Question'])

# Prepare DataLoader
test_dataloader = DataLoader(tokenized_test_dataset, batch_size=16)

# Function to evaluate the model with a progress bar
def evaluate_model(model, dataloader):
    device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
    model.to(device)
    model.eval()
    predictions, true_labels = [], []

    for batch in tqdm(dataloader, desc="Evaluating"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)

        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

    return predictions, true_labels

# Run evaluation
preds, labels = evaluate_model(model, test_dataloader)

# Calculate evaluation metrics
accuracy = accuracy_score(labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")


# Predict for the AIMO test set
tokenizer = AutoTokenizer.from_pretrained("lschlessinger/bert-finetuned-math-prob-classification")
model = AutoModelForSequenceClassification.from_pretrained("lschlessinger/bert-finetuned-math-prob-classification")
# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

test_df = pd.read_csv('../resources/AIMO/test.csv')
# test_df = pd.read_csv('ALLtraincompiled.csv').iloc[:100,:]

# Convert to Hugging Face dataset
test_dataset = Dataset.from_pandas(test_df)


label_mapping = {
    0: "Algebra",
    1: "Counting & Probability",
    2: "Geometry",
    3: "Intermediate Algebra",
    4: "Number Theory",
    5: "Prealgebra",
    6: "Precalculus"
}

def predict(text):
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    
    # Move inputs to GPU if available
    inputs = {key: val.to(device) for key, val in inputs.items()}
    
    # Make prediction
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Get the predicted class
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    predicted_class = torch.argmax(predictions, dim=-1).item()

    predicted_label = label_mapping[predicted_class]
    
    return predicted_label

# Apply the prediction function to the dataset
test_dataset = test_dataset.map(lambda x: {'Type prediction': predict(x['problem'])})
df = pd.DataFrame(test_dataset) 

df






# Load model directly
import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import DataLoader
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm

tokenizer = AutoTokenizer.from_pretrained("lschlessinger/bert-finetuned-math-prob-classification")
model = AutoModelForSequenceClassification.from_pretrained("lschlessinger/bert-finetuned-math-prob-classification")

test_df = pd.read_csv('../resources/test_questions_with_plain_text.csv')
# Convert 'Type' column to categorical if not already
test_df['Type'] = test_df['Type'].astype('category')

# Map the labels to integers
test_df['label'] = test_df['Type'].cat.codes

test_df = test_df[['label','Question']]
# Convert to Hugging Face dataset
test_dataset = Dataset.from_pandas(test_df)

# Function to tokenize data with truncation
def tokenize_data(examples):
    return tokenizer(examples['Question'], padding='max_length', truncation=True, max_length=512)

# Tokenize the datasets
tokenized_test_dataset = test_dataset.map(tokenize_data, batched=True, num_proc=4)
tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# classifier = pipeline("text-classification", model="lschlessinger/bert-finetuned-math-prob-classification")
# classifier(tokenized_datasets['test']['New_Question'])

# Prepare DataLoader
test_dataloader = DataLoader(tokenized_test_dataset, batch_size=16)

# Function to evaluate the model with a progress bar
def evaluate_model(model, dataloader):
    device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
    model.to(device)
    model.eval()
    predictions, true_labels = [], []

    for batch in tqdm(dataloader, desc="Evaluating"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)

        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

    return predictions, true_labels

# Run evaluation
preds, labels = evaluate_model(model, test_dataloader)

# Calculate evaluation metrics
accuracy = accuracy_score(labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")



